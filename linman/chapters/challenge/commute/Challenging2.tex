\documentclass[journal,12pt,twocolumn]{IEEEtran}

\usepackage{setspace}
\usepackage{gensymb}


\singlespacing

\usepackage[cmex10]{amsmath}
%\usepackage{amsthm}
%\interdisplaylinepenalty=2500
%\savesymbol{iint}
%\usepackage{txfonts}
%\restoresymbol{TXF}{iint}
%\usepackage{wasysym}
\usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}

\usepackage{longtable}
\usepackage{multirow}

\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{steinmetz}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage{tfrupee}
\usepackage[breaklinks=true]{hyperref}

\usepackage{tkz-euclide} %loads TikZ and tkz-base

\usetikzlibrary{calc,math}
\usepackage{listings}
    \usepackage{color}                                          
    \usepackage{array}                                          
    \usepackage{longtable}                                      
    \usepackage{calc}                                           
    \usepackage{multirow}                                       
    \usepackage{hhline}                                         
    \usepackage{ifthen}
    \usepackage{lscape}     
\usepackage{multicol}
\usepackage{chngcntr}

\DeclareMathOperator*{\Res}{Res}

\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}

\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}

\begin{document}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}

\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}

\bibliographystyle{IEEEtran}

\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
%\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}

\numberwithin{equation}{subsection}

\makeatletter
\@addtoreset{figure}{problem}
\makeatother

\let\StandardTheFigure\thefigure
\let\vec\mathbf

\renewcommand{\thefigure}{\theproblem}

\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}
\vspace{3cm}

\title{Challenging Problem 1}
\author{Surbhi Agarwal}

\maketitle

\newpage

%\tableofcontents

\bigskip

\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}

\begin{abstract}
This document looks at the cases when the matrix multiplication of two matrices is commutative.
\end{abstract}

Download latex-tikz codes from 
%
\begin{lstlisting}
https://github.com/surbhi0912/EE5609/tree/master/challenging_problem/challenging2
\end{lstlisting}
%
\section{Problem}
We know that in general, $\vec{A}\times\vec{B} \ne \vec{B}\times\vec{A}$, i.e. matrix multiplication is not commutative in general. What are the conditions on $\vec{A}$ and $\vec{B}$ such that $\vec{A}\times\vec{B} = \vec{B}\times\vec{A}$?

\section{Explanation}
Two matrices $\vec{A}$ and $\vec{B}$ commute on matrix multiplication if they are Simultaneously Diagonalizable.\\

An $n\times n$ matrix $\vec{A}$ is diagonalizable if and only if it has n linearly independent eigen vectors. Then, it is similar to a diagonal matrix and can be expressed as
\begin{align}\label{eq1}
    \vec{A} = \vec{P}\vec{D_1}\vec{P}^{-1}
\end{align}
for some invertible matrix $\vec{P}$ and diagonal matrix $\vec{D_1}$. Here, the columns of $\vec{P}$ are $n$ linearly independent eigen vectors of $\vec{A}$, and the diagonal entries of $\vec{D_1}$ are eigenvalues of $\vec{A}$ that correspond to respective eigen vectors in $\vec{P}$\\

A matrix $\vec{B}$ is said to be Simultaneously diagonalizable with $\vec{A}$ if the same $\vec{P}$ diagonalizes both matrices, so $\vec{B}$ can be expressed as
\begin{align}\label{eq2}
    \vec{B} = \vec{P}\vec{D_2}\vec{P}^{-1}
\end{align}
\section{Proof}
Using above concept, we see that
\begin{align}
    \vec{A}\times\vec{B} & = (\vec{P}\vec{D_1}\vec{P}^{-1})(\vec{P}\vec{D_2}\vec{P}^{-1})\\& =\vec{P}\vec{D_1}(\vec{P}^{-1}\vec{P})\vec{D_2}\vec{P}^{-1}\\& = \label{eq2.0.5} \vec{P}\vec{D_1}\vec{D_2}\vec{P}^{-1}
\end{align}
\begin{align}
    \vec{B}\times\vec{A} & = (\vec{P}\vec{D_2}\vec{P}^{-1})(\vec{P}\vec{D_1}\vec{P}^{-1})\\& =\vec{P}\vec{D_2}(\vec{P}^{-1}\vec{P})\vec{D_1}\vec{P}^{-1}\\& = \label{eq2.0.8} \vec{P}\vec{D_2}\vec{D_1}\vec{P}^{-1}
\end{align}
Now, note that since $\vec{D_1}$ and $\vec{D_2}$ are diagonal matrices,
\begin{align}\label{eq2.0.9}
    \vec{D_1}\vec{D_2} = \vec{D_2}\vec{D_1}
\end{align}
because the product of two diagonal matrices is the product of their corresponding diagonal elements, and multiplication for the diagonal elements is commutative since they are scalar.
From Equations \eqref{eq2.0.5},\eqref{eq2.0.8} and \eqref{eq2.0.9}, we conclude
\begin{align}
    \vec{A}\times\vec{B} = \vec{B}\times\vec{A}
\end{align}
Hence, proved the case where matrix multiplication is commutative.
\end{document}